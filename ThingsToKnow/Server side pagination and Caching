
Client side is when you pull all the data down and then the client segments the data into pages.
Server-side is usually done by the client providing a key that is passed to the server and then the server only selects out that "page" of data. For example if you were displaying people by last name, the first page might be created by telling the server that you want people with the last name of 'A' and that you want 10 rows returned.
The server would do something like:

SELECT ssn, fname, lname 
FROM people 
WHERE lname like 'a%' and rownum <= 10 ORDER BY lname, ssn;

If the last/10th record has a last name of 'abbot' with an SSN of 555555555,
then the next page could be retrieved by the client passing those values back 
to the server which would then do something such as:
SELECT ssn, fname, lname 
FROM people 
WHERE lname >= 'abbot' and ssn > 555555555 and rownum <= 10 ORDER BY lname, ssn;
Server-side is considered better for large-sets of data as the amount of data transferred to the
client is far smaller than if all the data is pulled down and "paged" by the client. 
It also lowers the memory required for the client side as well as taking advantage of
the strong capabilities of databases to sort data or use existing sorted indexes to speed selection. 

The server could tell the client how many pages there are so the client knows how many numbers to display. 
The server takes a number of parameters from the user (current page #, ordering, etc) and performs whatever search 
required to get just the relevant records. These are sent to the client, along with links to more pages, etc.




https://coderwall.com/p/lkcaag/pagination-you-re-probably-doing-it-wrong

Pagination? No problem! SELECT ... LIMIT X, Y Right?
Hah! Not quite!
You see, your pagination logic is not stable; and that matters quite a bit in this ajaxy, 
client-appy, infinite-scrolly world.

•	Don't paginate on row offset.
•	Find a stable value to paginate on, like an item's date.
•	Always deduplicate list items on the client side.
•	Never trust your server to know exactly what the user is looking at.

What are you going on about?
It's probably best to give an example. Let's say you're adding comments to a page, 
and you want the most recent comments to show up first. The naive approach is to construct a query similar to:

SELECT * FROM comments ORDER BY date DESC LIMIT 0, 10

Easy. That gives you the first 10 most recent comments. Nothing wrong here. Now, the user pages down and you perform the same query, except for LIMIT 10,10 - and you return the next 10 comments...
...sometimes.
Consider this:
•	User A is viewing a page with the first 10 comments loaded.
•	User B creates a comment.
•	User A loads the next page of comments.
Well, crap. Now User A sees a duplicate comment!
The last comment on their first page got pushed down by the new comment, and it became the 11th comment in the list. 
Now your page just looks sloppy ☹

A similar situation occurs if you allow for removal of items from a list, but with even more disastrous results.
Instead of the comments being pushed down, they get pulled up. 
The item that would have previously been first on the next page is shifted up; 
and User A would completely skip over it! That really sucks ☹ ☹

Instead of paginating on a volatile offset (the row number), find a value that is stable across time. 
In the comments example, that is the date of the comments themselves.
Take the example above: Your first query is the same. 
However, for the next page, instead of passing the row offset you pass the date of the last comment in the list:

SELECT * FROM comments WHERE date < prevous_date LIMIT 10

Perfect! Now you can be sure that the user will always see the very next set of commits that 
occurred after the one they are looking at.

What if you have two comments with the same date? Ack! Pagination is hard.
You've got a few options:
•	If you are using an auto incrementing identifier for your content, use that instead of the date; it's guaranteed to increase over time and behave nicely for these cases. 


If you don't have an auto incrementing id (e.g. most distributed data stores), then things are a bit more complicated:
•	Instead of finding comments that occurred <u>before</u> the date, 
find comments that occurred <u>at or before</u> that date. Then, make sure you filter out duplicate comments before 
displaying them to the user.
You can't rely on servers on what user sees. Javascript on client side you can filter out those duplicates. 




Caching and Pagination: 

how do you cache paginated content : 
https://medium.freecodecamp.org/how-to-implement-cacheable-pagination-of-frequently-changing-content-c8ddc8269e81

web applications heavily rely on the HTTP cache. Sometimes this kind of caching does not work well for paginated content.
If the collection is changed frequently and the content needs to be up-to-date, then it simply doesn’t make sense to 
cache the response of the page as the page content changes.

if we can fetch separately the order of the item IDs and the item details, then the second call can potentially be cached.
one call to fetch the ordering which will give only itemID. 
Second call goes to the cache to get the details of every item. - Caching item details results in reduced usage of bandwidth.
If item does not exist in cache we retieve the detail of the item. 

Unfortunately, such a split will multiply the amount of network calls by an order of magnitude.
A similar pair of calls will happen for each consecutive page. 
The main idea is that we can retrieve most item details from the cache. 
But unfortunately we have to request the order of item IDs for every page.

Instead of requesting 20–40 item IDs we can request 100-200. 
Later, the UI layer can moderate the number of item details that need to be displayed and request them accordingly.
Now adding a new item to the top still results in two requests (one for IDs and the other one for details of this new item).
But we won’t have to request the next page for a while, because the pages are bigger. 
We also won’t need to request item details because they are cached!



Twitter Uses cursors to pagination : 
https://developer.twitter.com/en/docs/basics/cursoring.html
where they have next and prev cusor points.


For the new entries we can have something that will trigger "new Content" on app like quora. 
then user clicks and we fetch the new content. 


